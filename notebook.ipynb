{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a935585",
   "metadata": {},
   "source": [
    "# Master’s Project – Advanced Deep Learning  \n",
    "# Industrial Sound Anomaly Detection using Transformers  \n",
    "\n",
    "**Authors:** \n",
    "**Date:** November 2025  \n",
    "\n",
    "\n",
    "**Chosen approach :**  \n",
    "Fine-tuning the **Audio Spectrogram Transformer (AST)** – MIT/ast-finetuned-audioset-10-10-0.4593  \n",
    "→ Currently the **best published model on MIMII**  \n",
    "→ 100 % compliant with the \"transformer required\" constraint  \n",
    "→ Extremely easy to implement with Hugging Face (< 50 lines)\n",
    "\n",
    "## 1. Problem Statement & Industrial Impact\n",
    "\n",
    "Modern factories want to **predict mechanical failures before they happen**.  \n",
    "Pumps, fans, valves, and sliding rails all produce a specific acoustic signature when healthy.  \n",
    "As soon as a defect appears (worn bearing, leakage, imbalance, etc.), the sound changes often **very subtly**.\n",
    "\n",
    "Goal of this project:  \n",
    "Build an intelligent model that **automatically detects these sound anomalies in real time** using **Transformer-based models**, currently the state-of-the-art for modeling long-term dependencies in audio signals.\n",
    "\n",
    "Real-world impact:\n",
    "- Prevents unplanned downtime \n",
    "- Increases safety\n",
    "- Enables predictive maintenance\n",
    "\n",
    "\n",
    "## 2. Dataset: MIMII (2021) – The standard benchmark\n",
    "\n",
    "Source: \n",
    "\n",
    "- 4 machine types: **fan**, **pump**, **valve**, **slider**\n",
    "- Real factory recordings (with natural background noise)\n",
    "- 10-second clips, 16 kHz, mono\n",
    "- Two classes: **normal** vs **anomalous** (multiple simulated faults)\n",
    "\n",
    "| Machine | Normal | Anomalous | Total   |\n",
    "|---------|--------|-----------|---------|\n",
    "| fan     | ~6,500 | ~2,000    | ~8,500  |\n",
    "| pump    | ~6,500 | ~2,000    | ~8,500  |\n",
    "| valve   | ~6,500 | ~2,000    | ~8,500  |\n",
    "| slider  | ~6,500 | ~2,000    | ~8,500  |\n",
    "| **Total**| **~26,000** | **~8,000** | **~34,000** |\n",
    "\n",
    "i\n",
    "\n",
    "## Final Strategy \n",
    "\n",
    "We focus on the **pump** machine only for training and validation:\n",
    "- Source domains (training)\n",
    "- Target domain (test only): never seen during training\n",
    "- Zero-shot transfer test: same model on **fan** \n",
    "\n",
    "This allows:\n",
    "- Very fast training \n",
    "- **cross-machine transfer** (fan → pump)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7159f2",
   "metadata": {},
   "source": [
    "## 3. Data Exploration & Visualization\n",
    "\n",
    "\n",
    "### 3.1 Data uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e9236c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e675c532b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "#sns.set(style=\"whitegrid\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72239c5e",
   "metadata": {},
   "source": [
    "# Uploading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2493fa",
   "metadata": {},
   "source": [
    "### online through zendoo website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb4c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'zenodo_get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zenodo-get\n",
      "  Downloading zenodo_get-2.0.0-py3-none-any.whl.metadata (46 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\melissa\\anaconda3\\envs\\torch\\lib\\site-packages (from zenodo-get) (2.32.5)\n",
      "Collecting wget (from zenodo-get)\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting humanize (from zenodo-get)\n",
      "  Downloading humanize-4.14.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting click (from zenodo-get)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting coverage>=7.8.2 (from zenodo-get)\n",
      "  Downloading coverage-7.12.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\melissa\\anaconda3\\envs\\torch\\lib\\site-packages (from click->zenodo-get) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\melissa\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->zenodo-get) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\melissa\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->zenodo-get) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\melissa\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->zenodo-get) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\melissa\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->zenodo-get) (2025.11.12)\n",
      "Downloading zenodo_get-2.0.0-py3-none-any.whl (34 kB)\n",
      "Downloading coverage-7.12.0-cp310-cp310-win_amd64.whl (220 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading humanize-4.14.0-py3-none-any.whl (132 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (pyproject.toml): started\n",
      "  Building wheel for wget (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9712 sha256=baeeccf028855e1b7a7af512d686b221841df04312ced0666ce1f45ce5b7dfbb\n",
      "  Stored in directory: c:\\users\\melissa\\appdata\\local\\pip\\cache\\wheels\\8b\\f1\\7f\\5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, humanize, coverage, click, zenodo-get\n",
      "\n",
      "   -------- ------------------------------- 1/5 [humanize]\n",
      "   -------- ------------------------------- 1/5 [humanize]\n",
      "   -------- ------------------------------- 1/5 [humanize]\n",
      "   -------- ------------------------------- 1/5 [humanize]\n",
      "   -------- ------------------------------- 1/5 [humanize]\n",
      "   -------- ------------------------------- 1/5 [humanize]\n",
      "   -------- ------------------------------- 1/5 [humanize]\n",
      "   ---------------- ----------------------- 2/5 [coverage]\n",
      "   ---------------- ----------------------- 2/5 [coverage]\n",
      "   ---------------- ----------------------- 2/5 [coverage]\n",
      "   ---------------- ----------------------- 2/5 [coverage]\n",
      "   ---------------- ----------------------- 2/5 [coverage]\n",
      "   ---------------- ----------------------- 2/5 [coverage]\n",
      "   ---------------- ----------------------- 2/5 [coverage]\n",
      "   ---------------- ----------------------- 2/5 [coverage]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   -------------------------------- ------- 4/5 [zenodo-get]\n",
      "   ---------------------------------------- 5/5 [zenodo-get]\n",
      "\n",
      "Successfully installed click-8.3.1 coverage-7.12.0 humanize-4.14.0 wget-3.2 zenodo-get-2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "# Install zenodo-get\n",
    "! pip install zenodo-get\n",
    "\n",
    "# Create data directory\n",
    "\n",
    "\n",
    "# Use python -m instead of calling the CLI directly\n",
    "! python -m zenodo_get -r 4740355 -o data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9edba",
   "metadata": {},
   "source": [
    "### getting it from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d76022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#DATA_ROOT = \"/content/drive/----/MIMII_DUE\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7357ec3",
   "metadata": {},
   "source": [
    "### if it is already installed in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset trouvé : True\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path('C:/Users/melissa/Desktop/Industrial Sound Anomaly Detection using Transformers/final_product')          \n",
    "print(\"Dataset found :\", DATA_ROOT.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6408d3c9",
   "metadata": {},
   "source": [
    "### modifying the sructur of the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d92365",
   "metadata": {},
   "source": [
    "actually the dataset  containes two kinds of folder those which start with **dev** followed by the name of the machine (e.g dev_data_fan)  and those that starts with **eval** (e.g eval_data_fan).\n",
    "\n",
    "the fist type of folder in itself contains a set of subfolders train,target-test, source-test ,each subfolder has s set of wave files representing different captured sounds of the said  machines.  now the problem is that the indication that those wave are anomalious or normal is actually written on the name of the wave file.\n",
    "\n",
    "so in order to make things easir for the upcoming  treatments we thought it will be best to restructure those folder and that's exactly what we did in this section :\n",
    "\n",
    "so for each machine folder we explore the sub ones and look for each wave file if amolious term figure on the name of file we put it in the anomalious folder else the normal one.\n",
    "\n",
    "in the end we've got a new folder **final product**  which is the one we are going to use .\n",
    "\n",
    "the new folder is structured like this :\n",
    "\n",
    "```\n",
    "\n",
    "final_dataset/\n",
    "    fan/\n",
    "        test/\n",
    "            anomalous/\n",
    "                section_00_source_test_anomaly_0000.wav\n",
    "            normal/\n",
    "                section_00_source_test_normal_0000.wav\n",
    "        train/\n",
    "            normal/\n",
    "                section_00_source_train_normal_0000_strength_1_ambient.wav\n",
    "    GearBox/\n",
    "    pump/\n",
    "    slider/\n",
    "    valve/\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528bd65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAN       → train: 3000 normal | test: 600 normal + 600 anomalous\n",
      "PUMP      → train: 3000 normal | test: 600 normal + 600 anomalous\n",
      "VALVE     → train: 3000 normal | test: 600 normal + 600 anomalous\n",
      "SLIDER    → train: 3000 normal | test: 610 normal + 604 anomalous\n",
      "GEARBOX   → train: 3017 normal | test: 720 normal + 687 anomalous\n",
      "TOY_CAR → folder not found: data\\dev_data_toy_car\\toy_car\n",
      "TOY_TRAIN → folder not found: data\\dev_data_toy_train\\toy_train\n",
      "\n",
      "FINAL_PRODUCT CREATED SUCCESSFULLY!\n",
      "Total → train normal: 15017 | test: 3130 normal + 3091 anomalous\n",
      "Folder: C:\\Users\\melissa\\Desktop\\Industrial Sound Anomaly Detection using Transformers\\final_product\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def create_final_product_dcase2021(raw_root=\"data\", final_root=\"final_dataset\"):\n",
    "    \"\"\"\n",
    "    Works with the official DCASE 2021 Task 2 / MIMII DUE structure:\n",
    "    dev_data_fan/fan/\n",
    "    dev_data_pump/pump/\n",
    "    ...\n",
    "    Creates clean unsupervised anomaly detection structure for ALL 7 machines.\n",
    "    \"\"\"\n",
    "    raw_root = Path(raw_root)\n",
    "    final_root = Path(final_root)\n",
    "    final_root.mkdir(exist_ok=True)\n",
    "\n",
    "    # All 7 machines \n",
    "    machine_map = {\n",
    "        \"fan\":       \"dev_data_fan/fan\",\n",
    "        \"pump\":      \"dev_data_pump/pump\", \n",
    "        \"valve\":     \"dev_data_valve/valve\",\n",
    "        \"slider\":    \"dev_data_slider/slider\",\n",
    "        \"gearbox\":   \"dev_data_gearbox/gearbox\",\n",
    "   \n",
    "    }\n",
    "\n",
    "    total_train_normal = 0\n",
    "    total_test_normal = 0\n",
    "    total_test_anomalous = 0\n",
    "\n",
    "    for machine_name, subfolder in machine_map.items():\n",
    "        machine_dir = raw_root / subfolder\n",
    "        if not machine_dir.exists():\n",
    "            print(f\"{machine_name.upper()} → folder not found: {machine_dir}\")\n",
    "            continue\n",
    "\n",
    "        out_dir = final_root / machine_name\n",
    "        (out_dir / \"train/normal\").mkdir(parents=True, exist_ok=True)\n",
    "        (out_dir / \"test/normal\").mkdir(parents=True, exist_ok=True)\n",
    "        (out_dir / \"test/anomalous\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        train_n = test_n = test_a = 0\n",
    "\n",
    "        for wav_file in machine_dir.rglob(\"*.wav\"):\n",
    "            name = wav_file.name.lower()\n",
    "\n",
    "            is_normal = \"normal\" in name\n",
    "            is_anomaly = \"anomaly\" in name or \"anomalous\" in name\n",
    "            is_source_train = \"source_train\" in name\n",
    "            is_test = \"source_test\" in name or \"target_test\" in name\n",
    "\n",
    "            if is_normal and is_source_train:\n",
    "                shutil.copy(wav_file, out_dir / \"train/normal\" / wav_file.name)\n",
    "                train_n += 1\n",
    "            elif is_normal and is_test:\n",
    "                shutil.copy(wav_file, out_dir / \"test/normal\" / wav_file.name)\n",
    "                test_n += 1\n",
    "            elif is_anomaly and is_test:\n",
    "                shutil.copy(wav_file, out_dir / \"test/anomalous\" / wav_file.name)\n",
    "                test_a += 1\n",
    "\n",
    "        print(f\"{machine_name.upper():9} → train: {train_n:4} normal | test: {test_n:3} normal + {test_a:3} anomalous\")\n",
    "        total_train_normal += train_n\n",
    "        total_test_normal += test_n\n",
    "        total_test_anomalous += test_a\n",
    "\n",
    "    print(\"\\nFINAL_DATASET CREATED SUCCESSFULLY!\")\n",
    "    print(f\"Total → train normal: {total_train_normal} | test: {total_test_normal} normal + {total_test_anomalous} anomalous\")\n",
    "    print(f\"Folder: {final_root.resolve()}\")\n",
    "\n",
    "# RUN THIS ONCE — creates everything perfectly\n",
    "create_final_product_dcase2021()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e1d40",
   "metadata": {},
   "source": [
    "### checking the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0e073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition finale – Pompe\n",
      "Train → 3,000 sons normaux (source_train)\n",
      "Test  → 600 normaux + 600 anormaux\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"final_dataset/pump\")\n",
    "print(\"finale repartition – Pompe\")\n",
    "print(f\"Train → {len(list((DATA_DIR/'train'/'normal').glob('*.wav'))):,} normal ones (source_train)\")\n",
    "print(f\"Test  → {len(list((DATA_DIR/'test'/'normal').glob('*.wav'))):,} normal ones + {len(list((DATA_DIR/'test'/'anomalous').glob('*.wav'))):,} anomalous ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e2ddd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
